{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e73d3715",
   "metadata": {},
   "source": [
    "### Evaluation of model using CUAD projects original metrics\n",
    "This is a modified version that allows to run the evaluation as a standalone version. Will be converted into a script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09b1750d-6fdb-4a8e-9982-40e833cca8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    AutoTokenizer,\n",
    "    squad_convert_examples_to_features\n",
    ")\n",
    "\n",
    "from transformers.data.processors.squad import SquadResult, SquadV2Processor, SquadExample\n",
    "from transformers.data.metrics.squad_metrics import compute_predictions_logits\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f057aa5-18e9-4e53-837e-136b8d30b629",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = SquadV2Processor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6148749c-0611-48e8-bce8-42479ce0ecf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 102/102 [01:39<00:00,  1.03it/s]\n"
     ]
    }
   ],
   "source": [
    "# Using the testfile from the original repo\n",
    "examples = processor.get_dev_examples('./', filename='./test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81a45c49-e36a-4b99-b72a-ac431f970df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path='roberta-base/'\n",
    "config = AutoConfig.from_pretrained(\n",
    "        model_path,\n",
    "        cache_dir=None,\n",
    "    )\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    do_lower_case=True,\n",
    "    use_fast=False,  # SquadDataset is not compatible with Fast tokenizers which have a smarter overflow handeling\n",
    ")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\n",
    "    model_path,\n",
    "    from_tf=bool(\".ckpt\" in model_path),\n",
    "    config=config,\n",
    "    cache_dir=None,\n",
    ")\n",
    "model_type=\"roberta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d62c9b7-54f0-495a-8e20-149f55f5b632",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "convert squad examples to features: 100%|██████████| 4182/4182 [1:28:19<00:00,  1.27s/it]\n",
      "add example index and unique id: 100%|██████████| 4182/4182 [00:00<00:00, 7611.60it/s] \n"
     ]
    }
   ],
   "source": [
    "doc_stride=128\n",
    "max_query_length=64\n",
    "max_seq_length=512\n",
    "evaluate=True\n",
    "threads=12\n",
    "\n",
    "# Very slow processing time (4-5 hours)\n",
    "features, dataset = squad_convert_examples_to_features(\n",
    "examples=examples,\n",
    "tokenizer=tokenizer,\n",
    "max_seq_length=max_seq_length,\n",
    "doc_stride=doc_stride,\n",
    "max_query_length=max_query_length,\n",
    "is_training=not evaluate,\n",
    "return_dataset=\"pt\",\n",
    "threads=threads,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8826815f-ac4f-4a2f-9832-22f61763baa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slow processing time\n",
    "torch.save({\"features\": features, \"dataset\": dataset, \"examples\": examples}, './datafeats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3d97d8c7-4d47-43e0-bbf4-ca456036992c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.load('./datafeats')\n",
    "features, dataset, examples = data.values()\n",
    "batch_size=32\n",
    "eval_sampler = SequentialSampler(dataset)\n",
    "eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2b028f-f368-4f54-b07b-846bcc177882",
   "metadata": {},
   "source": [
    "Evalutating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "92c39bfb-eb08-404c-bc8a-8735ef74e3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_list(tensor):\n",
    "    return tensor.detach().cpu().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a1f326af-d415-4c12-9552-0ac88b834d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Tesla T4\n",
      "Memory Usage:\n",
      "Allocated: 0.5 GB\n",
      "Cached:    0.8 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Determine cuda\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "\n",
    "model.to(device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ee8ac1bb-7bd8-4b39-93fc-6946e7ba035c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 19188/19188 [2:04:28<00:00,  2.57it/s]  \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "all_results = []\n",
    "\n",
    "for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "    model.eval()\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs = {\n",
    "            \"input_ids\": batch[0],\n",
    "            \"attention_mask\": batch[1],\n",
    "            \"token_type_ids\": batch[2],\n",
    "        }\n",
    "\n",
    "        if model_type in [\"xlm\", \"roberta\", \"distilbert\", \"camembert\", \"bart\", \"longformer\"]:\n",
    "            del inputs[\"token_type_ids\"]\n",
    "\n",
    "        feature_indices = batch[3]\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    for i, feature_index in enumerate(feature_indices):\n",
    "        eval_feature = features[feature_index.item()]\n",
    "        unique_id = int(eval_feature.unique_id)\n",
    "\n",
    "        # for own model\n",
    "        # start_logits, end_logits = outputs[0][i], outputs[1][i]\n",
    "        output = [to_list(output[i]) for output in outputs.to_tuple()]\n",
    "\n",
    "        # Some models (XLNet, XLM) use 5 arguments for their predictions, while the other \"simpler\"\n",
    "        # models only use two.\n",
    "        if len(output) >= 5:\n",
    "            start_logits = output[0]\n",
    "            start_top_index = output[1]\n",
    "            end_logits = output[2]\n",
    "            end_top_index = output[3]\n",
    "            cls_logits = output[4]\n",
    "\n",
    "            result = SquadResult(\n",
    "                unique_id,\n",
    "                start_logits,\n",
    "                end_logits,\n",
    "                start_top_index=start_top_index,\n",
    "                end_top_index=end_top_index,\n",
    "                cls_logits=cls_logits,\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            start_logits, end_logits = output\n",
    "            result = SquadResult(unique_id, start_logits, end_logits)\n",
    "\n",
    "        all_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a5b844a2-a2a1-47ef-a675-f224f2e9bcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "output_prediction_file = os.path.join(\"./\", \"predictions_{}.json\".format(model_type))\n",
    "output_nbest_file = os.path.join(\"./\", \"nbest_predictions_{}.json\".format(model_type))\n",
    "output_null_log_odds_file = os.path.join(\"./\", \"null_odds_{}.json\".format(model_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "745d807a-cb2e-41f8-a62e-b3f572c2cd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import compute_predictions_logits, squad_evaluate\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c25f301a-d5cc-463e-b06b-9fdbf16f690c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.json', \"r\") as f:\n",
    "        json_test_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "09b050f9-e175-4889-882c-5cb7955f6b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_best_size=3\n",
    "max_answer_length=512\n",
    "do_lower_case=True\n",
    "verbose_logging=False\n",
    "version_2_with_negative=True\n",
    "null_score_diff_threshold=0.0\n",
    "\n",
    "\n",
    "predictions = compute_predictions_logits(\n",
    "    json_test_dict,\n",
    "    examples,\n",
    "    features,\n",
    "    all_results,\n",
    "    n_best_size,\n",
    "    max_answer_length,\n",
    "    do_lower_case,\n",
    "    output_prediction_file,\n",
    "    output_nbest_file,\n",
    "    output_null_log_odds_file,\n",
    "    verbose_logging,\n",
    "    version_2_with_negative,\n",
    "    null_score_diff_threshold,\n",
    "    tokenizer,\n",
    ")\n",
    "\n",
    "# Compute the F1 and exact scores.\n",
    "results = squad_evaluate(examples, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eac3aabc-43aa-44af-8a8c-03d909232807",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\"pred\":predictions},'pred_roberta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4482a24e-d759-494c-830c-f98ce3a0965b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\"all_results\":all_results},'allres_roberta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "95ab12b2-41ca-4836-97dd-e7f8428f5575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('exact', 72.45337159253945),\n",
       "             ('f1', 75.93157427285503),\n",
       "             ('total', 4182),\n",
       "             ('HasAns_exact', 71.70418006430869),\n",
       "             ('HasAns_f1', 83.396980393151),\n",
       "             ('HasAns_total', 1244),\n",
       "             ('NoAns_exact', 72.7705922396188),\n",
       "             ('NoAns_f1', 72.7705922396188),\n",
       "             ('NoAns_total', 2938),\n",
       "             ('best_exact', 72.7403156384505),\n",
       "             ('best_exact_thresh', 0.0),\n",
       "             ('best_f1', 75.91719507089961),\n",
       "             ('best_f1_thresh', 0.0)])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e07cd1",
   "metadata": {},
   "source": [
    "## Getting the paper metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e685ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import cuad utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694b0794",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose=True\n",
    "name= \"roberta\"\n",
    "\n",
    "gt_dict = json_test_dict\n",
    "gt_dict = get_answers(gt_dict)\n",
    "\n",
    "# predictions_path = 'nbest_predictions_roberta.json'\n",
    "\n",
    "pred_dict = load_json(output_prediction_file)\n",
    "\n",
    "assert sorted(list(pred_dict.keys())) == sorted(list(gt_dict.keys()))\n",
    "\n",
    "precisions, recalls, confs = get_precisions_recalls(pred_dict, gt_dict)\n",
    "prec_at_90_recall, _ = get_prec_at_recall(precisions, recalls, confs, recall_thresh=0.9)\n",
    "prec_at_80_recall, _ = get_prec_at_recall(precisions, recalls, confs, recall_thresh=0.8)\n",
    "aupr = get_aupr(precisions, recalls)\n",
    "\n",
    "if verbose:\n",
    "    print(\"AUPR: {:.3f}, Precision at 80% Recall: {:.3f}, Precision at 90% Recall: {:.3f}\".format(aupr, prec_at_80_recall, prec_at_90_recall))\n",
    "\n",
    "# now save results as a dataframe and return\n",
    "\n",
    "results = {\"name\": name, \"aupr\": aupr, \"prec_at_80_recall\": prec_at_80_recall, \"prec_at_90_recall\": prec_at_90_recall}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7aa344",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c776be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-10.m89",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-10:m89"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
