{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of attention\n",
    "\n",
    "This notebooks dives into the attention layers of the model and looks of explanations of predictions. Furthermore, it seeks to understand if there is the expected connections between "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uses the bertviz lib\n",
    "\n",
    "Please note that the output os written to files because the size crashes most notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model View\n",
    "<b>The model view provides a birds-eye view of attention throughout the entire model</b>. Each cell shows the attention weights for a particular head, indexed by layer (row) and head (column).  The lines in each cell represent the attention from one token (left) to another (right), with line weight proportional to the attention value (ranges from 0 to 1).  For a more detailed explanation, please refer to the [blog](https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1).\n",
    "\n",
    "## Usage\n",
    "ðŸ‘‰ **Click** on any **cell** for a detailed view of attention for the associated attention head (or to unselect that cell). <br/>\n",
    "ðŸ‘‰ Then **hover** over any **token** on the left side of detail view to filter the attention from that token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Head View\n",
    "<b>The head view visualizes attention in one or more heads from a single Transformer layer.</b> Each line shows the attention from one token (left) to another (right). Line weight reflects the attention value (ranges from 0 to 1), while line color identifies the attention head. When multiple heads are selected (indicated by the colored tiles at the top), the corresponding  visualizations are overlaid onto one another.  For a more detailed explanation of attention in Transformer models, please refer to the [blog](https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1).\n",
    "\n",
    "## Usage\n",
    "ðŸ‘‰ **Hover** over any **token** on the left/right side of the visualization to filter attention from/to that token. <br/>\n",
    "ðŸ‘‰ **Double-click** on any of the **colored tiles** at the top to filter to the corresponding attention head.<br/>\n",
    "ðŸ‘‰ **Single-click** on any of the **colored tiles** to toggle selection of the corresponding attention head. <br/>\n",
    "ðŸ‘‰ **Click** on the **Layer** drop-down to change the model layer (zero-indexed).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQUAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  47\n",
      " 12.300061702728271\n"
     ]
    }
   ],
   "source": [
    "# Squad\n",
    "from transformers import RobertaModel, RobertaTokenizer, AutoModelForQuestionAnswering, AutoTokenizer\n",
    "from bertviz import head_view, model_view\n",
    "import torch\n",
    "\n",
    "\n",
    "model_version = 'deepset/roberta-base-squad2'\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_version, output_attentions=True)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_version)\n",
    "\n",
    "sentence_b = \"In 1872, the Central Pacific Railroad established a station near Easterby'sâ€”by now a hugely productive wheat farmâ€”for its new Southern Pacific line. Soon there was a store around the station and the store grew the town of Fresno Station, later called Fresno. Many Millerton residents, drawn by the convenience of the railroad and worried about flooding, moved to the new community. Fresno became an incorporated city in 1885. By 1931 the Fresno Traction Company operated 47 streetcars over 49 miles of track.\"\n",
    "sentence_a = 'How many streetcars did the Fresno Traction Company operate in 1931?'\n",
    "inputs = tokenizer.encode_plus(sentence_a, sentence_b, return_tensors='pt', add_special_tokens=True)\n",
    "input_ids = inputs['input_ids']\n",
    "outputs=model(input_ids)\n",
    "attention = outputs[-1]\n",
    "input_id_list = input_ids[0].tolist() # Batch index 0\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_id_list)\n",
    "answer_start_scores, answer_end_scores = outputs.start_logits, outputs.end_logits\n",
    "answer_start = torch.argmax(\n",
    "    answer_start_scores\n",
    ")  # Get the most likely beginning of answer with the argmax of the score\n",
    "answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score\n",
    "answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids.squeeze()[answer_start:answer_end]))\n",
    "\n",
    "print(f\"Answer: {answer}\\n {answer_start_scores.max().tolist()+answer_end_scores.max().tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_data = head_view(attention, tokens, input_id_list.index(2), html_action='return')\n",
    "with open(\"./Figures_For_report/head_view_squad.html\", 'w') as file:\n",
    "    file.write(html_data.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_data = model_view(attention, tokens, input_id_list.index(2), html_action='return')\n",
    "with open(\"./Figures_For_report/model_view_squad.html\", 'w') as file:\n",
    "    file.write(html_data.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUAD\n",
    "\n",
    "For ease of use the CUAD checkpoint is used, but this should be checkpoint from the thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  DISTRIBUTOR AGREEMENT\n",
      " 11.535788536071777\n"
     ]
    }
   ],
   "source": [
    "model_version = 'Rakib/roberta-base-on-cuad'\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_version, output_attentions=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_version)\n",
    "\n",
    "sentence_b = \"EXHIBIT 10.6 DISTRIBUTOR AGREEMENT THIS DISTRIBUTOR AGREEMENT is made by and between Electric City Corp., a Delaware corporation ('Company') and Electric City of Illinois LLC ('Distributor') this 7th day of September, 1999. RECITALS A.\"\n",
    "sentence_a = 'Highlight the parts (if any) of this contract related to \"Document Name\" that should be reviewed by a lawyer. Details: The name of the contract'\n",
    "inputs = tokenizer.encode_plus(sentence_a, sentence_b, return_tensors='pt', add_special_tokens=True)\n",
    "input_ids = inputs['input_ids']\n",
    "outputs=model(input_ids)\n",
    "attention = outputs[-1]\n",
    "input_id_list = input_ids[0].tolist() # Batch index 0\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_id_list)\n",
    "answer_start_scores, answer_end_scores = outputs.start_logits, outputs.end_logits\n",
    "answer_start = torch.argmax(\n",
    "    answer_start_scores\n",
    ")  # Get the most likely beginning of answer with the argmax of the score\n",
    "answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score\n",
    "answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids.squeeze()[answer_start:answer_end]))\n",
    "\n",
    "print(f\"Answer: {answer}\\n {answer_start_scores.max().tolist()+answer_end_scores.max().tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_data = head_view(attention, tokens, input_id_list.index(2), html_action='return')\n",
    "with open(\"./Figures_For_report/head_view_cuad.html\", 'w') as file:\n",
    "    file.write(html_data.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_data = model_view(attention, tokens, input_id_list.index(2), html_action='return')\n",
    "with open(\"./Figures_For_report/model_view_cuad.html\", 'w') as file:\n",
    "    file.write(html_data.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Legal bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at nlpaueb/legal-bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at nlpaueb/legal-bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model is not fine-tuned for QA\n"
     ]
    }
   ],
   "source": [
    "model_version = 'nlpaueb/legal-bert-base-uncased'\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_version, output_attentions=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_version)\n",
    "\n",
    "sentence_b = \"EXHIBIT 10.6 DISTRIBUTOR AGREEMENT THIS DISTRIBUTOR AGREEMENT is made by and between Electric City Corp., a Delaware corporation ('Company') and Electric City of Illinois LLC ('Distributor') this 7th day of September, 1999. RECITALS A.\"\n",
    "sentence_a = 'Highlight the parts (if any) of this contract related to \"Document Name\" that should be reviewed by a lawyer. Details: The name of the contract'\n",
    "inputs = tokenizer.encode_plus(sentence_a, sentence_b, return_tensors='pt', add_special_tokens=True)\n",
    "input_ids = inputs['input_ids']\n",
    "outputs=model(input_ids)\n",
    "attention = outputs[-1]\n",
    "input_id_list = input_ids[0].tolist() # Batch index 0\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_id_list)\n",
    "\n",
    "print(\"This model is not fine-tuned for QA\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_data = head_view(attention, tokens, input_id_list.index(102), html_action='return')\n",
    "with open(\"./Figures_For_report/head_view_legal.html\", 'w') as file:\n",
    "    file.write(html_data.data)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f263b30b47212596c6dca0cd91fb0c51f07ee748dab3d0cfddddb639cf5b7790"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('test4')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
